{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nfrom typing import Dict, List, Tuple\nimport gradio as gr\n\nclass QuestionAnsweringSystem:\n    def __init__(self, model_checkpoint: str = \"distilbert-base-cased-distilled-squad\"):\n        \"\"\"Initialize the QA system with a pre-trained model.\"\"\"\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n        self.model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint).to(self.device)\n        self.max_length = 384\n        self.stride = 128\n        self.n_best = 20\n        self.max_answer_length = 30\n\n    def preprocess_for_prediction(self, question: str, context: str) -> Dict:\n        \"\"\"Preprocess question and context for prediction.\"\"\"\n        inputs = self.tokenizer(\n            question,\n            context,\n            max_length=self.max_length,\n            truncation=\"only_second\",\n            stride=self.stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        return inputs\n\n    def predict_answer(self, question: str, context: str) -> Tuple[str, float]:\n        \"\"\"Predict answer for a given question and context.\"\"\"\n        # Preprocess inputs\n        inputs = self.preprocess_for_prediction(question, context)\n        \n        # Move to device\n        input_ids = inputs[\"input_ids\"].to(self.device)\n        attention_mask = inputs[\"attention_mask\"].to(self.device)\n        token_type_ids = inputs[\"token_type_ids\"].to(self.device) if \"token_type_ids\" in inputs else None\n        \n        # Get model predictions\n        with torch.no_grad():\n            if token_type_ids is not None:\n                outputs = self.model(input_ids=input_ids, \n                                   attention_mask=attention_mask,\n                                   token_type_ids=token_type_ids)\n            else:\n                outputs = self.model(input_ids=input_ids, \n                                   attention_mask=attention_mask)\n        \n        start_logits = outputs.start_logits.cpu().numpy()\n        end_logits = outputs.end_logits.cpu().numpy()\n        offset_mapping = inputs[\"offset_mapping\"]\n        \n        # Find best answer across all chunks\n        best_answer = \"\"\n        best_score = float('-inf')\n        \n        for i in range(len(start_logits)):\n            start_logit = start_logits[i]\n            end_logit = end_logits[i]\n            offsets = offset_mapping[i]\n            \n            # Get top predictions\n            start_indexes = np.argsort(start_logit)[-1:-self.n_best-1:-1].tolist()\n            end_indexes = np.argsort(end_logit)[-1:-self.n_best-1:-1].tolist()\n            \n            # Find sequence boundaries\n            sequence_ids = inputs.sequence_ids(i)\n            context_start = sequence_ids.index(1) if 1 in sequence_ids else 0\n            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1) if 1 in sequence_ids else len(sequence_ids) - 1\n            \n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip invalid answers\n                    if (start_index < context_start or \n                        end_index > context_end or\n                        start_index > end_index or\n                        end_index - start_index + 1 > self.max_answer_length):\n                        continue\n                    \n                    # Skip answers not in context\n                    if (offsets[start_index] is None or \n                        offsets[end_index] is None):\n                        continue\n                    \n                    # Calculate score and extract answer\n                    score = start_logit[start_index] + end_logit[end_index]\n                    if score > best_score:\n                        best_score = score\n                        answer_start = offsets[start_index][0]\n                        answer_end = offsets[end_index][1]\n                        best_answer = context[answer_start:answer_end]\n        \n        return best_answer, float(best_score)\n\n    def batch_predict(self, questions: List[str], contexts: List[str]) -> List[Tuple[str, float]]:\n        \"\"\"Predict answers for multiple question-context pairs.\"\"\"\n        results = []\n        for question, context in zip(questions, contexts):\n            answer, score = self.predict_answer(question, context)\n            results.append((answer, score))\n        return results\n\n# Initialize the QA system\nprint(\"Initializing QA System...\")\nqa_system = QuestionAnsweringSystem()\nprint(\"QA System ready!\")\n\ndef gradio_interface(question: str, context: str) -> str:\n    \"\"\"Gradio interface function.\"\"\"\n    if not question.strip() or not context.strip():\n        return \"Please provide both question and context.\"\n    \n    try:\n        answer, confidence = qa_system.predict_answer(question, context)\n        if not answer:\n            return \"I couldn't find an answer in the given context.\"\n        return f\"Answer: {answer}\\nConfidence Score: {confidence:.2f}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef create_gradio_app():\n    \"\"\"Create and launch Gradio app.\"\"\"\n    iface = gr.Interface(\n        fn=gradio_interface,\n        inputs=[\n            gr.Textbox(\n                label=\"Question\", \n                placeholder=\"Enter your question here...\",\n                lines=2\n            ),\n            gr.Textbox(\n                label=\"Context\", \n                placeholder=\"Enter the context/passage here...\",\n                lines=10\n            )\n        ],\n        outputs=gr.Textbox(label=\"Answer\", lines=3),\n        title=\"ü§ñ Question Answering System\",\n        description=\"Ask questions about any given text passage. The system will find the most relevant answer from the context.\",\n        examples=[\n            [\n                \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n                \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\n            ]\n        ],\n        theme=gr.themes.Soft()\n    )\n    return iface\n\n# Test the system\ndef test_qa_system():\n    \"\"\"Test function to verify the system works.\"\"\"\n    test_question = \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n    test_context = \"\"\"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\"\"\n    \n    try:\n        answer, confidence = qa_system.predict_answer(test_question, test_context)\n        print(f\"Test Question: {test_question}\")\n        print(f\"Test Answer: {answer}\")\n        print(f\"Confidence: {confidence:.2f}\")\n        return True\n    except Exception as e:\n        print(f\"Test failed with error: {e}\")\n        return False\n\n# Run test\nprint(\"\\nTesting the system...\")\nif test_qa_system():\n    print(\"‚úÖ System test passed!\")\n    \n    # Launch Gradio interface\n    print(\"\\nLaunching Gradio interface...\")\n    iface = create_gradio_app()\n    iface.launch(share=True)\nelse:\n    print(\"‚ùå System test failed!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T09:26:29.634364Z","iopub.execute_input":"2025-07-21T09:26:29.635207Z","iopub.status.idle":"2025-07-21T09:27:08.815440Z","shell.execute_reply.started":"2025-07-21T09:26:29.635178Z","shell.execute_reply":"2025-07-21T09:27:08.814565Z"}},"outputs":[{"name":"stdout","text":"Initializing QA System...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02510ea2793148f680e9eb23143e58b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634bd99d2a104110bde1ac8da541d53a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6231cffda6fa43daa4fc9d4d84a9c138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe9f9400af342ca9b83d418f41c5acc"}},"metadata":{}},{"name":"stderr","text":"2025-07-21 09:26:51.058203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753090011.293118      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753090011.362167      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefb13f733b041ffb6cc4cb61dc33c9b"}},"metadata":{}},{"name":"stdout","text":"QA System ready!\n\nTesting the system...\nTest Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nTest Answer: Saint Bernadette Soubirous\nConfidence: 26.12\n‚úÖ System test passed!\n\nLaunching Gradio interface...\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://701d5bbaf502a04728.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://701d5bbaf502a04728.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}